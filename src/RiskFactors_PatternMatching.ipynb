{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pattern matching\n",
    "### to answering \"What do we know about COVID-19 risk factors?\"\n",
    "\n",
    "This notebook contains a pattern matching approach applied to the stated question above."
   ]
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltkstopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_numeric, remove_stopwords, strip_multiple_whitespaces\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": [
    "base_path = \"../input/CORD-19-research-challenge\"\n",
    "\n",
    "biorxiv_medrxiv = \"biorxiv_medrxiv/biorxiv_medrxiv/pdf_json\"\n",
    "comm_use_subset = \"comm_use_subset/comm_use_subset/pdf_json\"\n",
    "noncomm_use_subset = \"noncomm_use_subset/noncomm_use_subset/pdf_json\"\n",
    "custom_license = \"custom_license/custom_license/pdf_json\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "files = []\n\ndef read_files(directory):\n    files_in_dir = [f for f in glob.glob(os.path.join(base_path, directory) + \"/*.json\", recursive=True)]\n    files.extend(files_in_dir)\n    raw_body_texts_of_file = []\n    for f in files_in_dir:\n        data = json.load(open(f))\n        #for key in data.keys():\n        #    print(\"{}: {}\\n\".format(key, data[key]))\n        \n        body_text = \"\"\n        for i in range(len(data[\"body_text\"])):\n            body_text += \" \" + data[\"body_text\"][i][\"text\"]\n        \n        body_text = re.sub(' +', ' ', body_text)\n        raw_body_texts_of_file.append(body_text)\n    return raw_body_texts_of_file\n\nraw_body_texts = []\nraw_body_texts.extend(read_files(biorxiv_medrxiv))\nraw_body_texts.extend(read_files(comm_use_subset))\n#raw_body_texts.extend(read_files(noncomm_use_subset))\n#raw_body_texts.extend(read_files(custom_license))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "def print_title(idx):\n    print(json.load(open(files[idx]))[\"metadata\"][\"title\"])\n\ndef print_text(filenumber, start_idx, end_idx):\n    text = raw_body_texts[filenumber]\n    start_idx = max(0, start_idx)\n    end_idx = min(len(text), end_idx)\n    print(text[start_idx:end_idx])\n    \ndef print_body_text(filenumber):\n    data = json.load(open(files[filenumber]))\n    body_text = \"\"\n    for i in range(len(data[\"body_text\"])):\n        body_text += \" \" + data[\"body_text\"][i][\"text\"]\n    body_text = re.sub(' +', ' ', body_text)\n    if len(body_text) > 1000:\n        print(body_text[:1000])\n    else:\n        print(body_text)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "print(\"Found {} raw body texts\".format(len(raw_body_texts)))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "NUMBER_OF_FILES = len(raw_body_texts)\nWINDOW_SIZE = 200",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Search papers by regex patterns\nThe regex patterns contain on one hand words which might appear in context of risk factors and on the other hand some risk factors."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "factor___risk_pattern = r\"(factor(.){0,9}risk)\" # for example for \"factors of risk\"\nrisk_factor_pattern = r\"(risk(.){0,4}factor)\" # for example risk factors\nrisk_pattern = r\"(risk)\"\nhigh_risk_pattern = r\"(high(.){0,6}risk)\"\ncomorbdit_pattern = r\"(comorbdit)\"\nco_infects_pattern = r\"(co(.){0,4}infect)\"\nneonat_pattern = r\"(neonat)\"\npregnant_pattern = r\"(pregnant)\"\nsmoking_pattern = r\"(smoking)\"\ncancer_pattern = r\"(cancer)\"\naverse_outcomes_pattern = r\"(advers(.){0,4}outcome)\"\n\nPATTERNS = [\n    factor___risk_pattern,\n    risk_factor_pattern,\n    #risk_pattern,\n    high_risk_pattern,\n    #comorbdit_pattern,\n    #co_infects_pattern,\n    #neonat_pattern,\n    #pregnant_pattern,\n    #smoking_pattern,\n    #cancer_pattern,\n    averse_outcomes_pattern\n]\n\nCUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation]\nCUSTOM_FILTERS_EXCLUDE_NUMERIC = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_numeric]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "%%time\ndef extract_windows_containing(pattern, print_out=True):\n    indices = []\n    preprocessed_texts = []\n    for idx in range(NUMBER_OF_FILES):\n        filtered_sentence = raw_body_texts[idx]\n        preprocessed_texts.append(filtered_sentence)\n        \n        indices_of_file = [(m.start(0), m.end(0)) for m in re.finditer(pattern, filtered_sentence)]\n        indices.append(indices_of_file)\n    \n    return indices, preprocessed_texts\n\n\nindices = [[] for _ in range(NUMBER_OF_FILES)]\nfor pattern in PATTERNS:\n    indices_, preprocessed_texts = extract_windows_containing(pattern)\n    for i in range(len(indices_)):\n        indices[i].extend(indices_[i])\n\nprint(\"Found {} candidates\".format(len([1 for a in indices if len(a)!=0])))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tokenize and remove stopwords"
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "%%time\ndef process_file(file_number, indices_of_file, filters):\n    tokenized_matches = []\n    for match in indices_of_file:\n        start = match[0]-WINDOW_SIZE\n        end = match[1]+WINDOW_SIZE\n        text = preprocessed_texts[file_number][start:end]\n        tokenized = preprocess_string(text, filters)\n        tokenized_matches.append(tokenized)\n\n    return tokenized_matches\n\ntokenized_data = []\nfor file_number, indices_of_file in enumerate(indices):\n    if len(indices_of_file) != 0:\n        for data in process_file(file_number, indices_of_file, CUSTOM_FILTERS_EXCLUDE_NUMERIC):\n            for word in data:\n                if len(word) > 2:\n                    tokenized_data.append(word)\n\nnltk_stop_words = set(nltkstopwords.words('english'))\nfor word in [\"high\", \"risk\", \"factor\", \"patients\", \"factors\", \"disease\"]:\n    nltk_stop_words.add(word)\nwithout_stopwords = [word for word in tokenized_data if word not in nltk_stop_words]\nprint(\"Number of words without stopwords: {}\\n with stopwords {}\".format(len(without_stopwords), len(tokenized_data)), end=\"\\n\\n\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Show words by count i.e. words which often appear in our current results\n\nWe can see some potential risk factors like  ('asthma', 2148) which means asthma appeared 2148 times in the context of potential words which indicates that a passage is about risk factors.\nHowever just looking at context words is not so useful, but we can use it as an intermediate step to further look at potential risk factors indentfied here."
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "counts = Counter(without_stopwords)\nprint(counts.most_common(500))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualization of the top words in a word cloud"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "text = \" \".join(without_stopwords)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "cell_type": "markdown",
   "source": "#### Search potential passages for given potential risks\n\nNow we are going through the preprocessed data and look for possible risks, which we collected through our knowledge and the experiment above.\n\nWe print the file number, the index where the inital match of the experiment above happens and an extract of the body text.\n\nBecause some patterns might match for the same window/extract of text we adjust the index to contain a bigger window to avoid intersecting text windows."
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "POTENTIAL_RISKS = [\n    \"smoking\",\n    \"pulmonary diseas\", \n    \"elder\",\n    \"diabetes\",\n    \"old\",\n    \"age\",\n    \"cancer\", \n    \"cardiac\",\n    \"cardio\"]",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "trusted": false
   },
   "cell_type": "code",
   "source": "def doit(only_directly_related_files=False):\n    print_counter = 0\n    overall_processed_indices_count = 0\n    for file_number, indices_of_file in enumerate(indices):\n        if only_directly_related_files and file_number in not_directly_related_files:\n            continue\n        if len(indices_of_file) != 0:\n            matches = process_file(file_number, indices_of_file, CUSTOM_FILTERS)\n            processed_indices_of_file = []\n            indices_of_file = sorted(indices_of_file)\n            # adjust indices\n            for i in range(len(indices_of_file)):\n                if i != 0 and len(processed_indices_of_file) != 0:\n                    if abs(processed_indices_of_file[-1][0] - indices_of_file[i][0]) > 100 and abs(processed_indices_of_file[-1][1] - indices_of_file[i][1]) > 100:\n                        processed_indices_of_file.append(indices_of_file[i])\n                    else:\n                        min_ = min(indices_of_file[i][0], processed_indices_of_file[-1][0])\n                        max_ = max(indices_of_file[i][1], processed_indices_of_file[-1][1])\n                        del processed_indices_of_file[-1]\n                        processed_indices_of_file.append((min_, max_))\n                else:\n                    processed_indices_of_file.append(indices_of_file[i])\n            overall_processed_indices_count += len(processed_indices_of_file)\n\n            for index, match in zip(processed_indices_of_file, matches):\n                for pattern in POTENTIAL_RISKS:\n                    if pattern in match:\n                        windows_size = max(int(len(\" \".join(match))/2), 300)\n                        if print_counter < 10:\n                            print(\"File number: {} index pair: {}\".format(file_number, index))\n                            print_text(file_number, index[0]-windows_size, index[1]+windows_size)\n                            print(\"\\n\\n\")\n                            print_counter += 1\n                        break\n    return overall_processed_indices_count\n\noverall_processed_indices_count = doit()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "print(\"Total number of possible text passages about risk factors: {}\".format(overall_processed_indices_count))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In total we found many possible text passages, where some are really about risk factors of covid 19 and others are not.\n\nAn example of a useful text extract is:\n\"\"\"\nFile number: 60 index pair: (12790, 12801)\nthe . https://doi.org/10.1101/2020.03.21.001586 doi: bioRxiv preprint Clinical investigations have suggested that patients with cardiac diseases, hypertension, or diabetes, who are treated with ACE2-increasing drugs including inhibitors and blockers show increased expression of ACE2 and thus are at higher risk of getting the SARS-CoV2 infection (26) . Also there are studies on the regulatory role of miRNA hsa-mir-27b-3p described in ACE2 Signaling (27) . The results of the present study suggest a strong correlation between miRNA hsa-mir-27b-3p and ACE2 which needs to be confirmed experimentally in SARS-C\n\"\"\"\n\nAnd one example of a bad extract with respect to risk factors is:\n\"\"\"\nFile number: 239 index pair: (199, 203)\n Severe cases of coronavirus disease 2019 (COVID-19) rapidly develop acute respiratory distress leading to respiratory failure, with high short-term mortality rates. At present, there is no reliable risk stratification tool for non-severe COVID-19 patients at admission. We aimed to construct an effective model for early identifying cases at high risk of progression to severe COVID-19. China were included retrospectively. All patients with non-severe COVID-19 during hospitalization were followed for\n\"\"\"\n\n\nIf we want to print more context for a given filenumber we can use the code in the next cell."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Show an extract based on start and end index for a file number"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "file_number = 239\nstart_idx = 0\nend_idx = 2000\n\nprint_text(file_number, start_idx, end_idx)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One drawback of this approach applied to the given data is the occurrence of false positives, this means our patterns match for passages which might be irrelevant.\nThis can for example happen when a document is not about covid 19 and hence the passage doesn't mention risk factors for covid 19. Maybe a document is about another virus, which is more or less similar to Covid 19, which makes it hard to judge if the paper is relevant.\nBelow we are looking for files which might not directly relate to Covid-19"
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "def is_file_directly_related(filenumber):\n    data = json.load(open(files[filenumber]))\n    synonyms_for_covid_19 = [\n        r\"(covid)\",\n        r\"(sars)\",\n        r\"(cov(.){0,4}2)\",\n        r\"(novo)\",\n        r\"(corona)\",\n    ]\n    body_text = \"\"\n    for i in range(len(data[\"body_text\"])):\n        body_text += \" \" + data[\"body_text\"][i][\"text\"].lower()\n    body_text = re.sub(' +', ' ', body_text)\n    \n    at_least_one_match = False\n    for synonym_pattern in synonyms_for_covid_19:\n        matches = [(m.start(0), m.end(0)) for m in re.finditer(synonym_pattern, body_text)]\n        if len(matches) != 0:\n            return True\n    return False\n\nnot_directly_related_files = []\nfor i in range(len(files)):\n    if not is_file_directly_related(i):\n        not_directly_related_files.append(i)\nprint(\"Found {} files which might not directly relate to Covid-19\".format(len(not_directly_related_files)))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### To investigate some of the potentially not directly related papers we can print their title and body text."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "print_title(not_directly_related_files[1])",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "print_body_text(not_directly_related_files[1])",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "overall_processed_indices_count = doit(only_directly_related_files=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "print(\"Total number of possible text passages about risk factors: {}\".format(overall_processed_indices_count))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another problem is, even if a paper at least contains the word \"coronavirus\" doesn't mean it is about it.\nFor example one paper only lists coronavirus as a coinfection, hence the risk factors in it are not about the coronavirus but are detected as potential risk factors by this pattern matching approach."
  },
  {
   "metadata": {
    "trusted": false
   },
   "cell_type": "code",
   "source": "",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}